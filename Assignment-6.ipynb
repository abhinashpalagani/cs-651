{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab897869-2ce9-44ea-9c03-0700fa03a064",
   "metadata": {},
   "source": [
    "# TEXT-BASED ANALYSIS CS-651-A\n",
    "# Abhinash Palagani\n",
    "# Assignment - 6 5/4/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e619f",
   "metadata": {},
   "source": [
    "## **TEXT SUMMERIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6774f3",
   "metadata": {},
   "source": [
    "**EXAMPLE TEXT-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3dc95a83-c038-4067-bc96-e95391a89b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d5803bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from googletrans import Translator, LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3f5546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "It was the best of times\n",
      "It was the worst of times\n",
      "It was the age of wisdom\n",
      "It was the age of foolishness\n",
      "What is the importance of age\n",
      "This is the best example\n",
      "\n",
      "Telugu Translations:\n",
      "ఇది ఉత్తమ సార్లు\n",
      "ఇది చాలా ఘోరమైనది\n",
      "ఇది వివేకం యొక్క యుగం\n",
      "ఇది మూర్ఖత్వం యొక్క యుగం\n",
      "వయస్సు యొక్క ప్రాముఖ్యత ఏమిటి\n",
      "ఇది ఉత్తమ ఉదాహరణ\n",
      "\n",
      "Hindi Translations:\n",
      "वह सबसे अच्छा समय था\n",
      "यह सबसे खराब समय था\n",
      "यह ज्ञान की उम्र थी\n",
      "यह मूर्खता की उम्र थी\n",
      "उम्र का महत्व क्या है\n",
      "यह सबसे अच्छा उदाहरण है\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from googletrans import Translator\n",
    "\n",
    "# Function to clean and translate sentences\n",
    "def process_and_translate(file_path):\n",
    "    # Open the file and read data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        filedata = file.readlines()\n",
    "\n",
    "    # Assuming the first paragraph is the first line, split by \". \"\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = [re.sub(\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in article if sentence.strip()]  # Clean each sentence\n",
    "\n",
    "    # Create a translator object\n",
    "    translator = Translator()\n",
    "\n",
    "    # Translate sentences to Telugu and Hindi\n",
    "    telugu_sentences = [translator.translate(sentence, dest='te').text for sentence in sentences if sentence]\n",
    "    hindi_sentences = [translator.translate(sentence, dest='hi').text for sentence in sentences if sentence]\n",
    "\n",
    "    # Print original and translated sentences\n",
    "    print(\"Original Sentences:\")\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "\n",
    "    print(\"\\nTelugu Translations:\")\n",
    "    for telugu_sentence in telugu_sentences:\n",
    "        print(telugu_sentence)\n",
    "\n",
    "    print(\"\\nHindi Translations:\")\n",
    "    for hindi_sentence in hindi_sentences:\n",
    "        print(hindi_sentence)\n",
    "\n",
    "# Path to the text file\n",
    "file_path = \"Text1.txt\"\n",
    "\n",
    "# Call the function\n",
    "process_and_translate(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f26212f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  ['I AM SAM', 'I AM SAM', 'SAM I AM', 'THAT SAMIAM THAT SAMIAM', 'I DO NOT LIKE THAT SAMIAM', 'DO WOULD YOU LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM', 'I DO NOT LIKE GREEN EGGS AND HAM', 'WOULD YOU LIKE THEM HERE OR THERE', 'I WOULD NOT LIKE THEM HERE OR THERE', 'I WOULD NOT LIKE THEM ANYWHERE', 'I DO NOT LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM', 'WOULD YOU LIKE THEM IN A HOUSE', 'WOULD YOU LIKE THEN WITH A MOUSE', 'I DO NOT LIKE THEM IN A HOUSE', 'I DO NOT LIKE THEM WITH A MOUSE', 'I DO NOT LIKE THEM HERE OR THERE', 'I DO NOT LIKE THEM ANYWHERE', 'I DO NOT LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "65db1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "723eb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         1.         1.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [1.         0.         1.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [1.         1.         0.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [0.84988905 0.84988905 0.84988905 0.         0.81584592 0.42319883\n",
      "  0.74941637 0.50448487 0.37238799 0.42878143 0.49851939 0.50448487\n",
      "  0.74941637 0.48575205 0.5139745  0.55464279 0.62229501 0.43694943\n",
      "  0.52319587 0.50448487 0.74941637]\n",
      " [0.78362778 0.78362778 0.78362778 0.81584592 0.         0.74864778\n",
      "  0.96980829 0.81876403 0.67767415 0.75705636 0.81280019 0.81876403\n",
      "  0.96980829 0.81311653 0.82417857 0.90247108 0.92387987 0.76076281\n",
      "  0.83832595 0.81876403 0.96980829]\n",
      " [0.54592083 0.54592083 0.54592083 0.42319883 0.74864778 0.\n",
      "  0.79462718 0.9469071  0.83262569 0.83106972 0.87332936 0.9469071\n",
      "  0.79462718 0.9016155  0.88083033 0.86686326 0.84072064 0.80618465\n",
      "  0.86067103 0.9469071  0.79462718]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  0.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  1.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 1.        ]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 0.         0.79019961 0.84430927 0.86903021 1.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 1.         0.85535961]\n",
      " [0.37062466 0.37062466 0.37062466 0.37238799 0.67767415 0.83262569\n",
      "  0.74535599 0.79019961 0.         0.97869033 0.92261291 0.79019961\n",
      "  0.74535599 0.88766705 0.88680311 0.84418042 0.85551835 0.9567917\n",
      "  0.91263344 0.79019961 0.74535599]\n",
      " [0.41403934 0.41403934 0.41403934 0.42878143 0.75705636 0.83106972\n",
      "  0.81131598 0.84430927 0.97869033 0.         0.9477582  0.84430927\n",
      "  0.81131598 0.88271667 0.88907405 0.900682   0.90777516 0.98885528\n",
      "  0.95322714 0.84430927 0.81131598]\n",
      " [0.49051147 0.49051147 0.49051147 0.49851939 0.81280019 0.87332936\n",
      "  0.85366559 0.86903021 0.92261291 0.9477582  0.         0.86903021\n",
      "  0.85366559 0.92615842 0.94311913 0.91796475 0.92452261 0.91550964\n",
      "  0.98045351 0.86903021 0.85366559]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 1.         0.79019961 0.84430927 0.86903021 0.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 1.         0.85535961]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  1.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  0.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 1.        ]\n",
      " [0.55405125 0.55405125 0.55405125 0.48575205 0.81311653 0.9016155\n",
      "  0.85259784 0.83721021 0.88766705 0.88271667 0.92615842 0.83721021\n",
      "  0.85259784 0.         0.98909397 0.94032469 0.93368014 0.8421149\n",
      "  0.89319179 0.83721021 0.85259784]\n",
      " [0.53526436 0.53526436 0.53526436 0.5139745  0.82417857 0.88083033\n",
      "  0.84983659 0.81906016 0.88680311 0.88907405 0.94311913 0.81906016\n",
      "  0.84983659 0.98909397 0.         0.93006948 0.94397516 0.8412953\n",
      "  0.9009746  0.81906016 0.84983659]\n",
      " [0.60324935 0.60324935 0.60324935 0.55464279 0.90247108 0.86686326\n",
      "  0.92897575 0.90683162 0.84418042 0.900682   0.91796475 0.90683162\n",
      "  0.92897575 0.94032469 0.93006948 0.         0.98005115 0.89984254\n",
      "  0.93870092 0.90683162 0.92897575]\n",
      " [0.6382437  0.6382437  0.6382437  0.62229501 0.92387987 0.84072064\n",
      "  0.95032764 0.869819   0.85551835 0.90777516 0.92452261 0.869819\n",
      "  0.95032764 0.93368014 0.94397516 0.98005115 0.         0.89888651\n",
      "  0.93434953 0.869819   0.95032764]\n",
      " [0.42192651 0.42192651 0.42192651 0.43694943 0.76076281 0.80618465\n",
      "  0.81589244 0.85187405 0.9567917  0.98885528 0.91550964 0.85187405\n",
      "  0.81589244 0.8421149  0.8412953  0.89984254 0.89888651 0.\n",
      "  0.9502684  0.85187405 0.81589244]\n",
      " [0.51479156 0.51479156 0.51479156 0.52319587 0.83832595 0.86067103\n",
      "  0.88098961 0.90035389 0.91263344 0.95322714 0.98045351 0.90035389\n",
      "  0.88098961 0.89319179 0.9009746  0.93870092 0.93434953 0.9502684\n",
      "  0.         0.90035389 0.88098961]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 1.         0.79019961 0.84430927 0.86903021 1.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 0.         0.85535961]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  1.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  1.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2f0e9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.04056870760369847, 1: 0.04056870760369847, 2: 0.04056870760369847, 3: 0.038165667836425395, 4: 0.050084017764484505, 5: 0.04774289033712994, 6: 0.051541662369715344, 7: 0.04928709681727173, 8: 0.04540534050605903, 9: 0.04756954017192708, 10: 0.04922282723923169, 11: 0.04928709681727173, 12: 0.051541662369715344, 13: 0.04904302234993975, 14: 0.04885662270744604, 15: 0.051044257607731465, 16: 0.0514456780268457, 17: 0.04729160977170593, 18: 0.049936125309016983, 19: 0.04928709681727174, 20: 0.05154166236971534}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d3dc0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.051541662369715344, 'I DO NOT LIKE THEM SAMIAM'), (0.051541662369715344, 'I DO NOT LIKE THEM SAMIAM'), (0.05154166236971534, 'I DO NOT LIKE THEM SAMIAM'), (0.0514456780268457, 'I DO NOT LIKE THEM WITH A MOUSE'), (0.051044257607731465, 'I DO NOT LIKE THEM IN A HOUSE'), (0.050084017764484505, 'I DO NOT LIKE THAT SAMIAM'), (0.049936125309016983, 'I DO NOT LIKE THEM ANYWHERE'), (0.04928709681727174, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04928709681727173, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04928709681727173, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04922282723923169, 'I WOULD NOT LIKE THEM ANYWHERE'), (0.04904302234993975, 'WOULD YOU LIKE THEM IN A HOUSE'), (0.04885662270744604, 'WOULD YOU LIKE THEN WITH A MOUSE'), (0.04774289033712994, 'DO WOULD YOU LIKE GREEN EGGS AND HAM'), (0.04756954017192708, 'I WOULD NOT LIKE THEM HERE OR THERE'), (0.04729160977170593, 'I DO NOT LIKE THEM HERE OR THERE'), (0.04540534050605903, 'WOULD YOU LIKE THEM HERE OR THERE'), (0.04056870760369847, 'SAM I AM'), (0.04056870760369847, 'I AM SAM'), (0.04056870760369847, 'I AM SAM'), (0.038165667836425395, 'THAT SAMIAM THAT SAMIAM')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0e5b2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "47aaa2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5f662",
   "metadata": {},
   "source": [
    "**EXAMPLE TEXT-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "93e86c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "aa24f4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "I AM SAM\n",
      "I AM SAM\n",
      "SAM I AM\n",
      "THAT SAMIAM THAT SAMIAM\n",
      "I DO NOT LIKE THAT SAMIAM\n",
      "DO WOULD YOU LIKE GREEN EGGS AND HAM\n",
      "I DO NOT LIKE THEM SAMIAM\n",
      "I DO NOT LIKE GREEN EGGS AND HAM\n",
      "WOULD YOU LIKE THEM HERE OR THERE\n",
      "I WOULD NOT LIKE THEM HERE OR THERE\n",
      "I WOULD NOT LIKE THEM ANYWHERE\n",
      "I DO NOT LIKE GREEN EGGS AND HAM\n",
      "I DO NOT LIKE THEM SAMIAM\n",
      "WOULD YOU LIKE THEM IN A HOUSE\n",
      "WOULD YOU LIKE THEN WITH A MOUSE\n",
      "I DO NOT LIKE THEM IN A HOUSE\n",
      "I DO NOT LIKE THEM WITH A MOUSE\n",
      "I DO NOT LIKE THEM HERE OR THERE\n",
      "I DO NOT LIKE THEM ANYWHERE\n",
      "I DO NOT LIKE GREEN EGGS AND HAM\n",
      "I DO NOT LIKE THEM SAMIAM\n",
      "\n",
      "Telugu Translations:\n",
      "నేను సామ్\n",
      "నేను సామ్\n",
      "సామ్ నేను\n",
      "ఆ సమియం ఆ సమియం\n",
      "నాకు ఆ సమియం నచ్చలేదు\n",
      "మీరు ఆకుపచ్చ గుడ్లు మరియు హామ్ చేయాలనుకుంటున్నారా\n",
      "నేను వాటిని సమియం ఇష్టపడను\n",
      "నాకు ఆకుపచ్చ గుడ్లు మరియు హామ్ ఇష్టం లేదు\n",
      "మీరు వాటిని ఇక్కడ లేదా అక్కడ కోరుకుంటున్నారా\n",
      "నేను ఇక్కడ లేదా అక్కడ వాటిని ఇష్టపడను\n",
      "నేను వాటిని ఎక్కడా ఇష్టపడను\n",
      "నాకు ఆకుపచ్చ గుడ్లు మరియు హామ్ ఇష్టం లేదు\n",
      "నేను వాటిని సమియం ఇష్టపడను\n",
      "మీరు వాటిని ఇంట్లో కోరుకుంటున్నారా\n",
      "మీరు ఎలుకతో ఇష్టపడుతున్నారా?\n",
      "నేను వాటిని ఇంట్లో ఇష్టపడను\n",
      "నేను వాటిని ఎలుకతో ఇష్టపడను\n",
      "నేను ఇక్కడ లేదా అక్కడ వాటిని ఇష్టపడను\n",
      "నేను వాటిని ఎక్కడా ఇష్టపడను\n",
      "నాకు ఆకుపచ్చ గుడ్లు మరియు హామ్ ఇష్టం లేదు\n",
      "నేను వాటిని సమియం ఇష్టపడను\n",
      "\n",
      "Hindi Translations:\n",
      "मैं सैम हूं\n",
      "मैं सैम हूं\n",
      "सैम मैं हूँ\n",
      "वह समियम कि समि\n",
      "मुझे वह समि पसंद नहीं है\n",
      "क्या आप हरे अंडे और हैम पसंद करेंगे\n",
      "मैं उन्हें samiam पसंद नहीं करता\n",
      "मुझे हरे अंडे और हैम पसंद नहीं है\n",
      "क्या आप उन्हें यहाँ या वहाँ पसंद करेंगे\n",
      "मैं उन्हें यहाँ या वहाँ पसंद नहीं करूंगा\n",
      "मैं उन्हें कहीं भी पसंद नहीं करूंगा\n",
      "मुझे हरे अंडे और हैम पसंद नहीं है\n",
      "मैं उन्हें samiam पसंद नहीं करता\n",
      "क्या आप उन्हें एक घर में पसंद करेंगे\n",
      "क्या आप तब एक माउस के साथ चाहेंगे\n",
      "मैं उन्हें एक घर में पसंद नहीं करता\n",
      "मैं उन्हें माउस के साथ पसंद नहीं करता\n",
      "मैं उन्हें यहाँ या वहाँ पसंद नहीं करता\n",
      "मैं उन्हें कहीं भी पसंद नहीं करता\n",
      "मुझे हरे अंडे और हैम पसंद नहीं है\n",
      "मैं उन्हें samiam पसंद नहीं करता\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from googletrans import Translator\n",
    "\n",
    "# Open the file and read data\n",
    "with open(\"Text2.txt\", \"r\") as file:\n",
    "    filedata = file.readlines()\n",
    "\n",
    "# Assume the first paragraph is the first line, split by \". \"\n",
    "article = filedata[0].split(\". \")\n",
    "sentences = [re.sub(\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in article if sentence.strip()]  # Clean each sentence\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "def translate_sentences(sentences, lang):\n",
    "    translations = [translator.translate(sentence, dest=lang).text for sentence in sentences if sentence]\n",
    "    return translations\n",
    "\n",
    "# Print original sentences\n",
    "print(\"Original Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Translate to Telugu\n",
    "telugu_sentences = translate_sentences(sentences, 'te')\n",
    "print(\"\\nTelugu Translations:\")\n",
    "for telugu_sentence in telugu_sentences:\n",
    "    print(telugu_sentence)\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_sentences = translate_sentences(sentences, 'hi')\n",
    "print(\"\\nHindi Translations:\")\n",
    "for hindi_sentence in hindi_sentences:\n",
    "    print(hindi_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ac25fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  ['I AM SAM', 'I AM SAM', 'SAM I AM', 'THAT SAMIAM THAT SAMIAM', 'I DO NOT LIKE THAT SAMIAM', 'DO WOULD YOU LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM', 'I DO NOT LIKE GREEN EGGS AND HAM', 'WOULD YOU LIKE THEM HERE OR THERE', 'I WOULD NOT LIKE THEM HERE OR THERE', 'I WOULD NOT LIKE THEM ANYWHERE', 'I DO NOT LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM', 'WOULD YOU LIKE THEM IN A HOUSE', 'WOULD YOU LIKE THEN WITH A MOUSE', 'I DO NOT LIKE THEM IN A HOUSE', 'I DO NOT LIKE THEM WITH A MOUSE', 'I DO NOT LIKE THEM HERE OR THERE', 'I DO NOT LIKE THEM ANYWHERE', 'I DO NOT LIKE GREEN EGGS AND HAM', 'I DO NOT LIKE THEM SAMIAM']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3efed690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "84f4d196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         1.         1.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [1.         0.         1.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [1.         1.         0.         0.84988905 0.78362778 0.54592083\n",
      "  0.79559225 0.59705025 0.37062466 0.41403934 0.49051147 0.59705025\n",
      "  0.79559225 0.55405125 0.53526436 0.60324935 0.6382437  0.42192651\n",
      "  0.51479156 0.59705025 0.79559225]\n",
      " [0.84988905 0.84988905 0.84988905 0.         0.81584592 0.42319883\n",
      "  0.74941637 0.50448487 0.37238799 0.42878143 0.49851939 0.50448487\n",
      "  0.74941637 0.48575205 0.5139745  0.55464279 0.62229501 0.43694943\n",
      "  0.52319587 0.50448487 0.74941637]\n",
      " [0.78362778 0.78362778 0.78362778 0.81584592 0.         0.74864778\n",
      "  0.96980829 0.81876403 0.67767415 0.75705636 0.81280019 0.81876403\n",
      "  0.96980829 0.81311653 0.82417857 0.90247108 0.92387987 0.76076281\n",
      "  0.83832595 0.81876403 0.96980829]\n",
      " [0.54592083 0.54592083 0.54592083 0.42319883 0.74864778 0.\n",
      "  0.79462718 0.9469071  0.83262569 0.83106972 0.87332936 0.9469071\n",
      "  0.79462718 0.9016155  0.88083033 0.86686326 0.84072064 0.80618465\n",
      "  0.86067103 0.9469071  0.79462718]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  0.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  1.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 1.        ]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 0.         0.79019961 0.84430927 0.86903021 1.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 1.         0.85535961]\n",
      " [0.37062466 0.37062466 0.37062466 0.37238799 0.67767415 0.83262569\n",
      "  0.74535599 0.79019961 0.         0.97869033 0.92261291 0.79019961\n",
      "  0.74535599 0.88766705 0.88680311 0.84418042 0.85551835 0.9567917\n",
      "  0.91263344 0.79019961 0.74535599]\n",
      " [0.41403934 0.41403934 0.41403934 0.42878143 0.75705636 0.83106972\n",
      "  0.81131598 0.84430927 0.97869033 0.         0.9477582  0.84430927\n",
      "  0.81131598 0.88271667 0.88907405 0.900682   0.90777516 0.98885528\n",
      "  0.95322714 0.84430927 0.81131598]\n",
      " [0.49051147 0.49051147 0.49051147 0.49851939 0.81280019 0.87332936\n",
      "  0.85366559 0.86903021 0.92261291 0.9477582  0.         0.86903021\n",
      "  0.85366559 0.92615842 0.94311913 0.91796475 0.92452261 0.91550964\n",
      "  0.98045351 0.86903021 0.85366559]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 1.         0.79019961 0.84430927 0.86903021 0.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 1.         0.85535961]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  1.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  0.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 1.        ]\n",
      " [0.55405125 0.55405125 0.55405125 0.48575205 0.81311653 0.9016155\n",
      "  0.85259784 0.83721021 0.88766705 0.88271667 0.92615842 0.83721021\n",
      "  0.85259784 0.         0.98909397 0.94032469 0.93368014 0.8421149\n",
      "  0.89319179 0.83721021 0.85259784]\n",
      " [0.53526436 0.53526436 0.53526436 0.5139745  0.82417857 0.88083033\n",
      "  0.84983659 0.81906016 0.88680311 0.88907405 0.94311913 0.81906016\n",
      "  0.84983659 0.98909397 0.         0.93006948 0.94397516 0.8412953\n",
      "  0.9009746  0.81906016 0.84983659]\n",
      " [0.60324935 0.60324935 0.60324935 0.55464279 0.90247108 0.86686326\n",
      "  0.92897575 0.90683162 0.84418042 0.900682   0.91796475 0.90683162\n",
      "  0.92897575 0.94032469 0.93006948 0.         0.98005115 0.89984254\n",
      "  0.93870092 0.90683162 0.92897575]\n",
      " [0.6382437  0.6382437  0.6382437  0.62229501 0.92387987 0.84072064\n",
      "  0.95032764 0.869819   0.85551835 0.90777516 0.92452261 0.869819\n",
      "  0.95032764 0.93368014 0.94397516 0.98005115 0.         0.89888651\n",
      "  0.93434953 0.869819   0.95032764]\n",
      " [0.42192651 0.42192651 0.42192651 0.43694943 0.76076281 0.80618465\n",
      "  0.81589244 0.85187405 0.9567917  0.98885528 0.91550964 0.85187405\n",
      "  0.81589244 0.8421149  0.8412953  0.89984254 0.89888651 0.\n",
      "  0.9502684  0.85187405 0.81589244]\n",
      " [0.51479156 0.51479156 0.51479156 0.52319587 0.83832595 0.86067103\n",
      "  0.88098961 0.90035389 0.91263344 0.95322714 0.98045351 0.90035389\n",
      "  0.88098961 0.89319179 0.9009746  0.93870092 0.93434953 0.9502684\n",
      "  0.         0.90035389 0.88098961]\n",
      " [0.59705025 0.59705025 0.59705025 0.50448487 0.81876403 0.9469071\n",
      "  0.85535961 1.         0.79019961 0.84430927 0.86903021 1.\n",
      "  0.85535961 0.83721021 0.81906016 0.90683162 0.869819   0.85187405\n",
      "  0.90035389 0.         0.85535961]\n",
      " [0.79559225 0.79559225 0.79559225 0.74941637 0.96980829 0.79462718\n",
      "  1.         0.85535961 0.74535599 0.81131598 0.85366559 0.85535961\n",
      "  1.         0.85259784 0.84983659 0.92897575 0.95032764 0.81589244\n",
      "  0.88098961 0.85535961 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0adfe007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.04056870760369847, 1: 0.04056870760369847, 2: 0.04056870760369847, 3: 0.038165667836425395, 4: 0.050084017764484505, 5: 0.04774289033712994, 6: 0.051541662369715344, 7: 0.04928709681727173, 8: 0.04540534050605903, 9: 0.04756954017192708, 10: 0.04922282723923169, 11: 0.04928709681727173, 12: 0.051541662369715344, 13: 0.04904302234993975, 14: 0.04885662270744604, 15: 0.051044257607731465, 16: 0.0514456780268457, 17: 0.04729160977170593, 18: 0.049936125309016983, 19: 0.04928709681727174, 20: 0.05154166236971534}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e2a5cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.051541662369715344, 'I DO NOT LIKE THEM SAMIAM'), (0.051541662369715344, 'I DO NOT LIKE THEM SAMIAM'), (0.05154166236971534, 'I DO NOT LIKE THEM SAMIAM'), (0.0514456780268457, 'I DO NOT LIKE THEM WITH A MOUSE'), (0.051044257607731465, 'I DO NOT LIKE THEM IN A HOUSE'), (0.050084017764484505, 'I DO NOT LIKE THAT SAMIAM'), (0.049936125309016983, 'I DO NOT LIKE THEM ANYWHERE'), (0.04928709681727174, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04928709681727173, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04928709681727173, 'I DO NOT LIKE GREEN EGGS AND HAM'), (0.04922282723923169, 'I WOULD NOT LIKE THEM ANYWHERE'), (0.04904302234993975, 'WOULD YOU LIKE THEM IN A HOUSE'), (0.04885662270744604, 'WOULD YOU LIKE THEN WITH A MOUSE'), (0.04774289033712994, 'DO WOULD YOU LIKE GREEN EGGS AND HAM'), (0.04756954017192708, 'I WOULD NOT LIKE THEM HERE OR THERE'), (0.04729160977170593, 'I DO NOT LIKE THEM HERE OR THERE'), (0.04540534050605903, 'WOULD YOU LIKE THEM HERE OR THERE'), (0.04056870760369847, 'SAM I AM'), (0.04056870760369847, 'I AM SAM'), (0.04056870760369847, 'I AM SAM'), (0.038165667836425395, 'THAT SAMIAM THAT SAMIAM')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "41ed4b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "39cb685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text in English:\n",
      "I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M.\n",
      "\n",
      "Summarized Text in Telugu:\n",
      "I d o n o t l i k e t h e m s a m i a M. i d o n o t l i k e t h e m s a m i a M. i d o n o t l i k e t h e m s a m i a M.\n",
      "\n",
      "Summarized Text in Hindi:\n",
      "I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M. I   D O   N O T   L I K E   T H E M   S A M I A M.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Assuming summarize_text is a list of strings (summarized sentences)\n",
    "# For example:\n",
    "# summarize_text = [\"This is the first summarized sentence\", \"This is the second summarized sentence\"]\n",
    "\n",
    "# Join the list into a single string, formatted as sentences\n",
    "summarized_content = \". \".join(summarize_text) + \".\"\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "def translate_summary(summary, language):\n",
    "    # Translate the summary to the specified language\n",
    "    translation = translator.translate(summary, dest=language).text\n",
    "    return translation\n",
    "\n",
    "# Print the English summary\n",
    "print(\"Summarized Text in English:\")\n",
    "print(summarized_content)\n",
    "\n",
    "# Translate and print the summary in Telugu\n",
    "telugu_summary = translate_summary(summarized_content, 'te')\n",
    "print(\"\\nSummarized Text in Telugu:\")\n",
    "print(telugu_summary)\n",
    "\n",
    "# Translate and print the summary in Hindi\n",
    "hindi_summary = translate_summary(summarized_content, 'hi')\n",
    "print(\"\\nSummarized Text in Hindi:\")\n",
    "print(hindi_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c9e2f",
   "metadata": {},
   "source": [
    "**EXAMPLE TEXT-3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "af334cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a54c2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "As an institution of higher learning Sacred Heart University places special emphasis on academic integrity which is a commitment to the fundamental values of honesty trust fairness respect and responsibility\n",
      "Only when these values are widely respected and practiced by all members of the University students faculty administrators and staff can the University maintain a culture that promotes free exploration of knowledge constructive debate genuine learning effective research fair assessment of student progress and development of members characters\n",
      "These aims of the University require that its members exercise mutual responsibilities\n",
      "At its core academic integrity is secured by a principled commitment to carry out these responsibilities not by rules and penalties\n",
      "Students and faculty should strive to create an academic environment that is honest fair and respectful of all\n",
      "They do this by evaluating others work fairly by responding to others ideas critically yet courteously by respecting others intellectual and physical property and by nurturing the values of academic integrity in all contexts of University life\n",
      "Appropriate disciplinary action will be taken for violations of academic integrity including plagiarism cheating any use of materials for an assignment or exam that is not permitted by the instructor and theft or mutilation of intellectual materials or other University equipment\n",
      "Faculty will assign failing grades for violations of the University policy on academic integrity and students may immediately receive an F for a course in which they commit a violation\n",
      "Violations of academic integrity are kept on file second violations will bring additional sanctions up to dismissal from the University\n",
      "For any disciplinary action the University affords the student the right of due process in an appeals procedure\n",
      "All matriculated students will be provided with a full description of the University standards for academic integrity consequences for violations and the appeals procedure\n",
      "\n",
      "Telugu Translations:\n",
      "ఉన్నత అభ్యాస పవిత్ర హృదయ విశ్వవిద్యాలయం యొక్క సంస్థగా విద్యా సమగ్రతకు ప్రత్యేక ప్రాధాన్యతనిస్తుంది, ఇది నిజాయితీ ట్రస్ట్ ఫెయిర్‌నెస్ గౌరవం మరియు బాధ్యత యొక్క ప్రాథమిక విలువలకు నిబద్ధత\n",
      "ఈ విలువలు విస్తృతంగా గౌరవించబడుతున్నప్పుడు మరియు విశ్వవిద్యాలయ విద్యార్థుల సభ్యులందరూ ప్రాక్టీస్ చేసినప్పుడు, విశ్వవిద్యాలయం విశ్వవిద్యాలయం ఒక సంస్కృతిని నిర్వహించవచ్చు, ఇది జ్ఞానం యొక్క ఉచిత అన్వేషణను ప్రోత్సహించే నిర్మాణాత్మక చర్చను నిజమైన అభ్యాసం సమర్థవంతమైన పరిశోధనల పరిశోధన న్యాయమైన పరిశోధన విద్యార్థుల పురోగతి మరియు సభ్యుల పాత్రల అభివృద్ధి\n",
      "విశ్వవిద్యాలయం యొక్క ఈ లక్ష్యాలకు దాని సభ్యులు పరస్పర బాధ్యతలను అందించాలి\n",
      "దాని ప్రధాన విద్యా సమగ్రత వద్ద నియమాలు మరియు జరిమానాల ద్వారా కాకుండా ఈ బాధ్యతలను నిర్వహించడానికి సూత్రప్రాయమైన నిబద్ధత ద్వారా భద్రపరచబడుతుంది\n",
      "విద్యార్థులు మరియు అధ్యాపకులు నిజాయితీగల న్యాయమైన మరియు అందరి గౌరవప్రదమైన విద్యా వాతావరణాన్ని సృష్టించడానికి ప్రయత్నించాలి\n",
      "ఇతరులను ఇతరులకు ప్రతిస్పందించడం ద్వారా ఇతరులను ఇతరులకు ప్రతిస్పందించడం ద్వారా వారు దీన్ని చేస్తారు, ఇతరులకు మేధో మరియు భౌతిక ఆస్తిని గౌరవించడం ద్వారా మరియు విశ్వవిద్యాలయ జీవితంలోని అన్ని సందర్భాలలో విద్యా సమగ్రత యొక్క విలువలను పెంపొందించడం ద్వారా విమర్శనాత్మకంగా ఇంకా మర్యాదపూర్వకంగా ఆలోచనలు చేస్తారు.\n",
      "అకాడెమిక్ సమగ్రత యొక్క ఉల్లంఘన కోసం తగిన క్రమశిక్షణా చర్యలు తీసుకోబడతాయి.\n",
      "విద్యా సమగ్రతపై విశ్వవిద్యాలయ విధానం యొక్క ఉల్లంఘనల కోసం ఫ్యాకల్టీ విఫలమైన తరగతులను కేటాయిస్తారు మరియు విద్యార్థులు వెంటనే ఒక కోర్సు కోసం ఎఫ్ పొందవచ్చు, దీనిలో వారు ఉల్లంఘనకు పాల్పడతారు\n",
      "విద్యా సమగ్రత యొక్క ఉల్లంఘనలు ఫైల్ రెండవ ఉల్లంఘనలపై ఉంచబడతాయి విశ్వవిద్యాలయం నుండి తొలగింపుకు అదనపు ఆంక్షలు తెస్తాయి\n",
      "ఏదైనా క్రమశిక్షణా చర్య కోసం విశ్వవిద్యాలయం విద్యార్థికి అప్పీల్స్ విధానంలో తగిన ప్రక్రియ యొక్క హక్కును అందిస్తుంది\n",
      "మెట్రిక్యులేటెడ్ విద్యార్థులందరికీ ఉల్లంఘనలు మరియు అప్పీల్ విధానం కోసం విద్యా సమగ్రత పరిణామాల కోసం విశ్వవిద్యాలయ ప్రమాణాల పూర్తి వివరణ ఇవ్వబడుతుంది\n",
      "\n",
      "Hindi Translations:\n",
      "उच्च शिक्षा के एक संस्था के रूप में पवित्र हृदय विश्वविद्यालय शैक्षणिक अखंडता पर विशेष जोर देता है जो ईमानदारी ट्रस्ट फेयरनेस सम्मान और जिम्मेदारी के मूल मूल्यों के लिए एक प्रतिबद्धता है\n",
      "केवल तभी जब इन मूल्यों का व्यापक रूप से सम्मान किया जाता है और विश्वविद्यालय के छात्रों के सभी सदस्यों द्वारा संकाय प्रशासक और कर्मचारियों द्वारा अभ्यास किया जाता है।\n",
      "विश्वविद्यालय के इन उद्देश्यों के लिए आवश्यक है कि इसके सदस्य आपसी जिम्मेदारियों का उपयोग करें\n",
      "इसके मूल शैक्षणिक अखंडता में इन जिम्मेदारियों को नियमों और दंडों द्वारा नहीं करने के लिए एक राजसी प्रतिबद्धता द्वारा सुरक्षित किया जाता है\n",
      "छात्रों और संकाय को एक शैक्षणिक वातावरण बनाने का प्रयास करना चाहिए जो ईमानदार निष्पक्ष और सभी का सम्मान करे\n",
      "वे दूसरों का मूल्यांकन करके दूसरों के विचारों का जवाब देकर काम करते हैं, दूसरों के विचारों को गंभीर रूप से अभी तक दूसरों को बौद्धिक और भौतिक संपत्ति का सम्मान करके और विश्वविद्यालय जीवन के सभी संदर्भों में शैक्षणिक अखंडता के मूल्यों का पोषण करके\n",
      "शैक्षणिक अखंडता के उल्लंघन के लिए उचित अनुशासनात्मक कार्रवाई की जाएगी, जिसमें साहित्यिक चोरी सहित किसी भी असाइनमेंट या परीक्षा के लिए सामग्री के किसी भी उपयोग को धोखा देना है जो प्रशिक्षक द्वारा अनुमति नहीं है और बौद्धिक सामग्री या अन्य विश्वविद्यालय उपकरणों की चोरी या उत्परिवर्तन\n",
      "संकाय अकादमिक अखंडता पर विश्वविद्यालय नीति के उल्लंघन के लिए असफल ग्रेड असाइन करेगा और छात्रों को तुरंत एक पाठ्यक्रम के लिए एक एफ प्राप्त हो सकता है जिसमें वे एक उल्लंघन करते हैं\n",
      "शैक्षणिक अखंडता के उल्लंघन को फाइल पर रखा जाता है दूसरा उल्लंघन विश्वविद्यालय से बर्खास्तगी के लिए अतिरिक्त प्रतिबंध लाएगा\n",
      "किसी भी अनुशासनात्मक कार्रवाई के लिए विश्वविद्यालय छात्र को अपील प्रक्रिया में नियत प्रक्रिया का अधिकार देता है\n",
      "सभी मैट्रिक किए गए छात्रों को उल्लंघन और अपील प्रक्रिया के लिए शैक्षणिक अखंडता परिणामों के लिए विश्वविद्यालय मानकों का पूर्ण विवरण प्रदान किया जाएगा\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from googletrans import Translator\n",
    "\n",
    "# Open the file and read data\n",
    "with open(\"Text3.txt\", \"r\") as file:\n",
    "    filedata = file.readlines()\n",
    "\n",
    "# Assume the first paragraph is the first line, split by \". \"\n",
    "article = filedata[0].split(\". \")\n",
    "sentences = [re.sub(\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in article if sentence.strip()]  # Clean each sentence\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "def translate_sentences(sentences, lang):\n",
    "    translations = [translator.translate(sentence, dest=lang).text for sentence in sentences if sentence]\n",
    "    return translations\n",
    "\n",
    "# Print original sentences\n",
    "print(\"Original Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Translate to Telugu\n",
    "telugu_sentences = translate_sentences(sentences, 'te')\n",
    "print(\"\\nTelugu Translations:\")\n",
    "for telugu_sentence in telugu_sentences:\n",
    "    print(telugu_sentence)\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_sentences = translate_sentences(sentences, 'hi')\n",
    "print(\"\\nHindi Translations:\")\n",
    "for hindi_sentence in hindi_sentences:\n",
    "    print(hindi_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6cccb579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  ['As an institution of higher learning Sacred Heart University places special emphasis on academic integrity which is a commitment to the fundamental values of honesty trust fairness respect and responsibility', 'Only when these values are widely respected and practiced by all members of the University students faculty administrators and staff can the University maintain a culture that promotes free exploration of knowledge constructive debate genuine learning effective research fair assessment of student progress and development of members characters', 'These aims of the University require that its members exercise mutual responsibilities', 'At its core academic integrity is secured by a principled commitment to carry out these responsibilities not by rules and penalties', 'Students and faculty should strive to create an academic environment that is honest fair and respectful of all', 'They do this by evaluating others work fairly by responding to others ideas critically yet courteously by respecting others intellectual and physical property and by nurturing the values of academic integrity in all contexts of University life', 'Appropriate disciplinary action will be taken for violations of academic integrity including plagiarism cheating any use of materials for an assignment or exam that is not permitted by the instructor and theft or mutilation of intellectual materials or other University equipment', 'Faculty will assign failing grades for violations of the University policy on academic integrity and students may immediately receive an F for a course in which they commit a violation', 'Violations of academic integrity are kept on file second violations will bring additional sanctions up to dismissal from the University', 'For any disciplinary action the University affords the student the right of due process in an appeals procedure', 'All matriculated students will be provided with a full description of the University standards for academic integrity consequences for violations and the appeals procedure']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b9605aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ddba9fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.96395605 0.92304712 0.97272191 0.96266307 0.96021428\n",
      "  0.9740433  0.96357256 0.95263437 0.96720721 0.96550547]\n",
      " [0.96395605 0.         0.91629147 0.95657541 0.97176814 0.95314201\n",
      "  0.94192801 0.92323543 0.89746673 0.96228696 0.96937655]\n",
      " [0.92304712 0.91629147 0.         0.93111882 0.85558865 0.88767841\n",
      "  0.88426546 0.85855895 0.8508645  0.88086185 0.88804344]\n",
      " [0.97272191 0.95657541 0.93111882 0.         0.93748867 0.9684236\n",
      "  0.96533009 0.9501769  0.941426   0.95728279 0.96516863]\n",
      " [0.96266307 0.97176814 0.85558865 0.93748867 0.         0.9501502\n",
      "  0.95321413 0.94017287 0.91916083 0.95886273 0.97269121]\n",
      " [0.96021428 0.95314201 0.88767841 0.9684236  0.9501502  0.\n",
      "  0.96668165 0.96146096 0.93750999 0.96755431 0.96746721]\n",
      " [0.9740433  0.94192801 0.88426546 0.96533009 0.95321413 0.96668165\n",
      "  0.         0.97541655 0.96759383 0.9544748  0.96463099]\n",
      " [0.96357256 0.92323543 0.85855895 0.9501769  0.94017287 0.96146096\n",
      "  0.97541655 0.         0.9748018  0.94350188 0.95469075]\n",
      " [0.95263437 0.89746673 0.8508645  0.941426   0.91916083 0.93750999\n",
      "  0.96759383 0.9748018  0.         0.92680909 0.95059999]\n",
      " [0.96720721 0.96228696 0.88086185 0.95728279 0.95886273 0.96755431\n",
      "  0.9544748  0.94350188 0.92680909 0.         0.97510175]\n",
      " [0.96550547 0.96937655 0.88804344 0.96516863 0.97269121 0.96746721\n",
      "  0.96463099 0.95469075 0.95059999 0.97510175 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2caf8ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.09231175341541512, 1: 0.09106989764016234, 2: 0.08625913624852172, 3: 0.09181537064106864, 4: 0.09078176800630978, 5: 0.09160150066041622, 6: 0.09182818716626664, 7: 0.09098039275197341, 8: 0.0899281596201335, 9: 0.09138229747447049, 10: 0.09204153637526227}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f3e66708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.09231175341541512, 'As an institution of higher learning Sacred Heart University places special emphasis on academic integrity which is a commitment to the fundamental values of honesty trust fairness respect and responsibility'), (0.09204153637526227, 'All matriculated students will be provided with a full description of the University standards for academic integrity consequences for violations and the appeals procedure'), (0.09182818716626664, 'Appropriate disciplinary action will be taken for violations of academic integrity including plagiarism cheating any use of materials for an assignment or exam that is not permitted by the instructor and theft or mutilation of intellectual materials or other University equipment'), (0.09181537064106864, 'At its core academic integrity is secured by a principled commitment to carry out these responsibilities not by rules and penalties'), (0.09160150066041622, 'They do this by evaluating others work fairly by responding to others ideas critically yet courteously by respecting others intellectual and physical property and by nurturing the values of academic integrity in all contexts of University life'), (0.09138229747447049, 'For any disciplinary action the University affords the student the right of due process in an appeals procedure'), (0.09106989764016234, 'Only when these values are widely respected and practiced by all members of the University students faculty administrators and staff can the University maintain a culture that promotes free exploration of knowledge constructive debate genuine learning effective research fair assessment of student progress and development of members characters'), (0.09098039275197341, 'Faculty will assign failing grades for violations of the University policy on academic integrity and students may immediately receive an F for a course in which they commit a violation'), (0.09078176800630978, 'Students and faculty should strive to create an academic environment that is honest fair and respectful of all'), (0.0899281596201335, 'Violations of academic integrity are kept on file second violations will bring additional sanctions up to dismissal from the University'), (0.08625913624852172, 'These aims of the University require that its members exercise mutual responsibilities')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a27979f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bbaa3981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text in English:\n",
      "A s   a n   i n s t i t u t i o n   o f   h i g h e r   l e a r n i n g   S a c r e d   H e a r t   U n i v e r s i t y   p l a c e s   s p e c i a l   e m p h a s i s   o n   a c a d e m i c   i n t e g r i t y   w h i c h   i s   a   c o m m i t m e n t   t o   t h e   f u n d a m e n t a l   v a l u e s   o f   h o n e s t y   t r u s t   f a i r n e s s   r e s p e c t   a n d   r e s p o n s i b i l i t y. A l l   m a t r i c u l a t e d   s t u d e n t s   w i l l   b e   p r o v i d e d   w i t h   a   f u l l   d e s c r i p t i o n   o f   t h e   U n i v e r s i t y   s t a n d a r d s   f o r   a c a d e m i c   i n t e g r i t y   c o n s e q u e n c e s   f o r   v i o l a t i o n s   a n d   t h e   a p p e a l s   p r o c e d u r e. A p p r o p r i a t e   d i s c i p l i n a r y   a c t i o n   w i l l   b e   t a k e n   f o r   v i o l a t i o n s   o f   a c a d e m i c   i n t e g r i t y   i n c l u d i n g   p l a g i a r i s m   c h e a t i n g   a n y   u s e   o f   m a t e r i a l s   f o r   a n   a s s i g n m e n t   o r   e x a m   t h a t   i s   n o t   p e r m i t t e d   b y   t h e   i n s t r u c t o r   a n d   t h e f t   o r   m u t i l a t i o n   o f   i n t e l l e c t u a l   m a t e r i a l s   o r   o t h e r   U n i v e r s i t y   e q u i p m e n t.\n",
      "\n",
      "Summarized Text in Telugu:\n",
      "A s a n i n s t i t i t i o n o f h i g h e r l e a r n i n g s a c r e d h e a r t u n i v e r s i t y p l a c e s p e c i a l e m p h a s o n a c a d e m i c i n t e r i t t t t t a l v a l u e s o f h o n e s t y t r u s t f a i r n e s s r e s p e c t a n d r e s p o n s i b i l i t y.A l l m a t r i c u l a t e d s t u d e n t s w i l l b e p r o v i d e d w i t h a f u l l d e s c r i p t i o f t h e u n i v e r s i t y s t a n d s r d s r a r a c a d e r i r i r n s a n d t h e a p p e a l s p r o c e d u r e.A p p r o p r i a t e d i s c i p l i n a r y a c t i o n w i l l b e t a k e n f o r v i o l a t i o n s o f a d e m i c i n t e g r i t y i n c l u d i n g p l a g i r i r i r i r i r i r i r e r e r e r e r e r e r e r e r e r i g n m e n t o r e x a m t h a t i s n o t p e r m i t t e d b y t h e i n s t r u c t o r a n d t h e f t o r m u t i l a t i o n o f i n t e l l e c t u a l m a t e r i a l s e r u n i r u n i r\n",
      "\n",
      "Summarized Text in Hindi:\n",
      "A S A N I N S T I T U T U T I O N O N O H H I G H E R L E R R N I N I N G S A C R E D H E R T U N N I V e r S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S। a l v a l u e s o o o o o o e s t y t r u s t t f a i r n e s s r e s s p e e e e s t a n d r e s p o n n s i b i l i t t y।A L L M A T R I C U L A T E D S T U D E D E N T S W I L L B E P R R O V I D E D D W I T H A F U L L L D E S C R I P T I T I o o o o e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e n s a n d t h e a p p e a l s p r o c e d u r e e।A P P r r o p r i a t e d i s s c i p l i n n a r y a c t i o n w i l l l l b e t t k e n o o r v o o o o o o o o o o o o o o o o o o o e e e e e e e e e e e e e e e e r r r r r r r e r r r r r I g n m e e n t o r e e x a m t h a t i s n o t p e r m i t t e d b y t h e i n s t r u u t o o r a n d t h e t o o r m m u t i l a t i o o o o o e e e o r o o r o o r o o r o o r o o r o o r o o r o o r r o o r o o r o o r o o r r o o r r o r r o\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Assuming summarize_text is a list of strings (summarized sentences)\n",
    "# summarize_text = [\"This is the first summarized sentence\", \"This is the second summarized sentence\"]\n",
    "\n",
    "# Join the list into a single string with proper sentence ending\n",
    "summarized_content = \". \".join(summarize_text) + \".\"\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "def translate_summary(summary, language):\n",
    "    # Translate the summary to the specified language\n",
    "    translation = translator.translate(summary, dest=language).text\n",
    "    return translation\n",
    "\n",
    "# Print the English summary\n",
    "print(\"Summarized Text in English:\")\n",
    "print(summarized_content)\n",
    "\n",
    "# Translate and print the summary in Telugu\n",
    "telugu_summary = translate_summary(summarized_content, 'te')\n",
    "print(\"\\nSummarized Text in Telugu:\")\n",
    "print(telugu_summary)\n",
    "\n",
    "# Translate and print the summary in Hindi\n",
    "hindi_summary = translate_summary(summarized_content, 'hi')\n",
    "print(\"\\nSummarized Text in Hindi:\")\n",
    "print(hindi_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae0bc3",
   "metadata": {},
   "source": [
    "**EXAMPLE TEXT-4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c7d201d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f9536322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "Imagine theres no heaven\n",
      "Its easy if you try\n",
      "No hell below us\n",
      "Above us only sky\n",
      "Imagine all the people livin for today\n",
      "Imagine theres no countries\n",
      "It isnt hard to do\n",
      "Nothing to kill or die for and no religion too\n",
      "Imagine all the people livin life in peace\n",
      "You may say Im a dreamer but Im not the only one\n",
      "I hope someday youll join us and the world will be as one\n",
      "Imagine no possessions\n",
      "I wonder if you can\n",
      "No need for greed or hunger\n",
      "A brotherhood of man\n",
      "Imagine all the people sharing all the world\n",
      "\n",
      "Telugu Translations:\n",
      "స్వర్గం లేదు అని వుహించుకో\n",
      "నువ్వు ప్రయత్నిస్తే ఇది సులభం\n",
      "మాకు క్రింద నరకం లేదు\n",
      "మాకు పైన ఆకాశం మాత్రమే\n",
      "ఈ రోజు నివసించే ప్రజలందరినీ g హించుకోండి\n",
      "దేశాలు లేవని g హించుకోండి\n",
      "ఇది చేయటం కష్టం కాదు\n",
      "చంపడానికి లేదా చనిపోవడానికి ఏమీ లేదు మరియు మతం కూడా లేదు\n",
      "ప్రజలందరూ శాంతితో జీవితాన్ని గడుపుతారు\n",
      "మీరు నేను కలలు కనేవాడిని అని చెప్పవచ్చు కాని నేను మాత్రమే కాదు\n",
      "ఏదో ఒక రోజు మీరు మాతో చేరతారని మరియు ప్రపంచం ఒకటిగా ఉంటుందని నేను ఆశిస్తున్నాను\n",
      "ఆస్తులను g హించుకోండి\n",
      "మీరు చేయగలిగితే నేను ఆశ్చర్యపోతున్నాను\n",
      "దురాశ లేదా ఆకలి అవసరం లేదు\n",
      "ఎ బ్రదర్హుడ్ ఆఫ్ మ్యాన్\n",
      "ప్రజలందరూ ప్రపంచమంతా పంచుకుంటారని g హించుకోండి\n",
      "\n",
      "Hindi Translations:\n",
      "कल्पना करो कोई स्वर्ग नहीं है\n",
      "अगर तुम कोशिश करो तो यह आसान है\n",
      "हमारे नीचे कोई नर्क नहीं\n",
      "हमारे ऊपर सिर्फ आकाश है\n",
      "आज के लिए सभी लोगों की कल्पना करें\n",
      "कल्पना करें कि कोई देश नहीं है\n",
      "यह करना मुश्किल नहीं है\n",
      "मारने या मरने के लिए कुछ भी नहीं है और कोई धर्म भी नहीं है\n",
      "सभी लोगों को शांति से जीवन की कल्पना कीजिए\n",
      "आप मुझे एक सपने देखने वाला कह सकते हैं लेकिन मैं केवल अकेला ऐसा नहीं हूँ\n",
      "मुझे उम्मीद है कि किसी दिन आप हमारे साथ जुड़ेंगे और दुनिया एक के रूप में होगी\n",
      "बिना किसी अधिकार के कल्पना करें\n",
      "में सोच रहा था की क्या आप ऐसा कर सकते हैं\n",
      "लालच या भूख की कोई ज़रूरत नहीं\n",
      "मानव का बन्धुत्व\n",
      "सभी लोगों को सारी दुनिया साझा करने की कल्पना करें\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from googletrans import Translator\n",
    "\n",
    "# Open the file and read data\n",
    "with open(\"Text4.txt\", \"r\") as file:\n",
    "    filedata = file.readlines()\n",
    "\n",
    "# Assume the first paragraph is the first line\n",
    "article = filedata[0].split(\". \")  # Split sentences by \". \"\n",
    "sentences = [re.sub(\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in article if sentence.strip()]  # Clean each sentence\n",
    "\n",
    "# Create a translator object\n",
    "translator = Translator()\n",
    "\n",
    "# Function to translate sentences\n",
    "def translate_sentences(sentences, lang):\n",
    "    translations = [translator.translate(sentence, dest=lang).text for sentence in sentences if sentence]\n",
    "    return translations\n",
    "\n",
    "# Print original sentences\n",
    "print(\"Original Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Translate to Telugu\n",
    "telugu_sentences = translate_sentences(sentences, 'te')\n",
    "print(\"\\nTelugu Translations:\")\n",
    "for telugu_sentence in telugu_sentences:\n",
    "    print(telugu_sentence)\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_sentences = translate_sentences(sentences, 'hi')\n",
    "print(\"\\nHindi Translations:\")\n",
    "for hindi_sentence in hindi_sentences:\n",
    "    print(hindi_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2616f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  ['Imagine theres no heaven', 'Its easy if you try', 'No hell below us', 'Above us only sky', 'Imagine all the people livin for today', 'Imagine theres no countries', 'It isnt hard to do', 'Nothing to kill or die for and no religion too', 'Imagine all the people livin life in peace', 'You may say Im a dreamer but Im not the only one', 'I hope someday youll join us and the world will be as one', 'Imagine no possessions', 'I wonder if you can', 'No need for greed or hunger', 'A brotherhood of man', 'Imagine all the people sharing all the world']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "12038ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b1903873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.5616528  0.60616789 0.56599996 0.79336554 0.91364077\n",
      "  0.58789635 0.67504003 0.820627   0.74403037 0.72219089 0.68262568\n",
      "  0.66811815 0.78179629 0.63181765 0.79406085]\n",
      " [0.5616528  0.         0.51220567 0.76459339 0.70413516 0.68094431\n",
      "  0.75299329 0.66229595 0.591233   0.83385216 0.75215349 0.57186964\n",
      "  0.75211832 0.53196479 0.55176059 0.61627907]\n",
      " [0.60616789 0.51220567 0.         0.75501199 0.75221667 0.61394061\n",
      "  0.51826749 0.71044134 0.71074232 0.69043096 0.88215569 0.53033009\n",
      "  0.63936201 0.65292863 0.61295206 0.8087458 ]\n",
      " [0.56599996 0.76459339 0.75501199 0.         0.67248264 0.59884947\n",
      "  0.57306826 0.66202625 0.55995023 0.79779064 0.8296855  0.64993368\n",
      "  0.70214689 0.57155819 0.61588176 0.62300202]\n",
      " [0.79336554 0.70413516 0.75221667 0.67248264 0.         0.78607041\n",
      "  0.74040835 0.85680987 0.94251272 0.83685926 0.89308086 0.61111629\n",
      "  0.7888512  0.69665787 0.69608764 0.94575017]\n",
      " [0.91364077 0.68094431 0.61394061 0.59884947 0.78607041 0.\n",
      "  0.70814208 0.77987784 0.77475022 0.76702748 0.74851143 0.79072143\n",
      "  0.75447384 0.78899311 0.65446289 0.75660479]\n",
      " [0.58789635 0.75299329 0.51826749 0.57306826 0.74040835 0.70814208\n",
      "  0.         0.85017005 0.57607341 0.78345482 0.7789622  0.55934964\n",
      "  0.76101943 0.60158521 0.74438737 0.68240017]\n",
      " [0.67504003 0.66229595 0.71044134 0.66202625 0.85680987 0.77987784\n",
      "  0.85017005 0.         0.72134389 0.79747545 0.85756716 0.64010118\n",
      "  0.86316869 0.77144987 0.80130718 0.77597361]\n",
      " [0.820627   0.591233   0.71074232 0.55995023 0.94251272 0.77475022\n",
      "  0.57607341 0.72134389 0.         0.72439398 0.80404105 0.59231548\n",
      "  0.70818957 0.63348419 0.49788682 0.90874702]\n",
      " [0.74403037 0.83385216 0.69043096 0.79779064 0.83685926 0.76702748\n",
      "  0.78345482 0.79747545 0.72439398 0.         0.89896489 0.56957649\n",
      "  0.84541554 0.72796197 0.77525531 0.7842181 ]\n",
      " [0.72219089 0.75215349 0.88215569 0.8296855  0.89308086 0.74851143\n",
      "  0.7789622  0.85756716 0.80404105 0.89896489 0.         0.65279121\n",
      "  0.85854571 0.75011961 0.7523049  0.88046202]\n",
      " [0.68262568 0.57186964 0.53033009 0.64993368 0.61111629 0.79072143\n",
      "  0.55934964 0.64010118 0.59231548 0.56957649 0.65279121 0.\n",
      "  0.59594633 0.51298918 0.48995593 0.56233848]\n",
      " [0.66811815 0.75211832 0.63936201 0.70214689 0.7888512  0.75447384\n",
      "  0.76101943 0.86316869 0.70818957 0.84541554 0.85854571 0.59594633\n",
      "  0.         0.75901411 0.71873832 0.68944179]\n",
      " [0.78179629 0.53196479 0.65292863 0.57155819 0.69665787 0.78899311\n",
      "  0.60158521 0.77144987 0.63348419 0.72796197 0.75011961 0.51298918\n",
      "  0.75901411 0.         0.74242587 0.69624803]\n",
      " [0.63181765 0.55176059 0.61295206 0.61588176 0.69608764 0.65446289\n",
      "  0.74438737 0.80130718 0.49788682 0.77525531 0.7523049  0.48995593\n",
      "  0.71873832 0.74242587 0.         0.66671071]\n",
      " [0.79406085 0.61627907 0.8087458  0.62300202 0.94575017 0.75660479\n",
      "  0.68240017 0.77597361 0.90874702 0.7842181  0.88046202 0.56233848\n",
      "  0.68944179 0.69624803 0.66671071 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c0f69023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.06196791315095241, 1: 0.058397909416202855, 2: 0.05916644924310871, 3: 0.05893209800879603, 4: 0.06782931870436759, 5: 0.06485034184195695, 6: 0.0602913831267919, 7: 0.06637861843456339, 8: 0.062021345211433494, 9: 0.06714314753893874, 10: 0.06958977985072251, 11: 0.05424770735038242, 12: 0.06476113554860015, 13: 0.060294411041876425, 14: 0.05895024087066662, 15: 0.06517820066063967}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ec1355eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.06958977985072251, 'I hope someday youll join us and the world will be as one'), (0.06782931870436759, 'Imagine all the people livin for today'), (0.06714314753893874, 'You may say Im a dreamer but Im not the only one'), (0.06637861843456339, 'Nothing to kill or die for and no religion too'), (0.06517820066063967, 'Imagine all the people sharing all the world'), (0.06485034184195695, 'Imagine theres no countries'), (0.06476113554860015, 'I wonder if you can'), (0.062021345211433494, 'Imagine all the people livin life in peace'), (0.06196791315095241, 'Imagine theres no heaven'), (0.060294411041876425, 'No need for greed or hunger'), (0.0602913831267919, 'It isnt hard to do'), (0.05916644924310871, 'No hell below us'), (0.05895024087066662, 'A brotherhood of man'), (0.05893209800879603, 'Above us only sky'), (0.058397909416202855, 'Its easy if you try'), (0.05424770735038242, 'Imagine no possessions')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8b9d1f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9c93cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " I   h o p e   s o m e d a y   y o u l l   j o i n   u s   a n d   t h e   w o r l d   w i l l   b e   a s   o n e. I m a g i n e   a l l   t h e   p e o p l e   l i v i n   f o r   t o d a y. Y o u   m a y   s a y   I m   a   d r e a m e r   b u t   I m   n o t   t h e   o n l y   o n e\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152d82a",
   "metadata": {},
   "source": [
    "**EXAMPLE TEXT-5:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9e14b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1c3c5cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "In an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills\n",
      "Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services\n",
      "As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses\n",
      "The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transIn an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills\n",
      "Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services\n",
      "As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses\n",
      "The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transforming lives and industry and the jobs of tomorrow will require a different skillset\n",
      "This will require more collaborations and training and working with AI\n",
      "Thats why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies\n",
      "The program is an attempt to ramp up the institutional setup and build capabilities among the educators to educate the workforce of tomorrow The program aims to build up the cognitive skills and indepth understanding of developing intelligent cloud connected solutions for applications across industry\n",
      "Earlier in April this year the company announced Microsoft Professional Program In AI as a learning track open to the public\n",
      "The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured handson labs and expert instructors as well\n",
      "This program also included developerfocused AI school that provided a bunch of assets to help build AI skills\n",
      "\n",
      "Telugu Translations:\n",
      "ఒక ఐషియాడీ వర్క్‌ఫోర్స్‌ను నిర్మించే ప్రయత్నంలో మైక్రోసాఫ్ట్ ఇంటెలిజెంట్ క్లౌడ్ హబ్‌ను ప్రకటించింది, ఇది తరువాతి తరం విద్యార్థులను ఎయిడీ స్కిల్స్ తో శక్తివంతం చేయడానికి ప్రారంభించబడింది\n",
      "త్రీఇయర్ సహకార కార్యక్రమంగా vision హించిన ఇంటెలిజెంట్ క్లౌడ్ హబ్ AI ఇన్ఫ్రాస్ట్రక్చర్ కోర్సు కంటెంట్ మరియు కరికులం డెవలపర్ డెవలప్‌మెంట్ డెవలప్‌మెంట్ సాధనాలతో సంస్థల చుట్టూ మద్దతు ఇస్తుంది మరియు విద్యార్థులకు క్లౌడ్ మరియు AI సేవలకు ప్రాప్తిని ఇస్తుంది\n",
      "ఈ కార్యక్రమంలో భాగంగా రెడ్‌మండ్ దిగ్గజం దాని పరిధిని విస్తరించాలని కోరుకుంటాడు మరియు ఈ కార్యక్రమంతో భారతదేశంలో బలమైన డెవలపర్ పర్యావరణ వ్యవస్థను నిర్మించాలని యోచిస్తోంది\n",
      "మనీష్ ప్రకాష్ కంట్రీ జనరల్ మేనేజర్ హెల్త్ అండ్ ఎడ్యుకేషన్‌కు మైక్రోసాఫ్ట్ కాగ్నిటివ్ సర్వీసెస్ బోట్ సర్వీసెస్ మరియు అజూర్ మెషిన్ లెర్నింగ్ఆర్కోర్డింగ్ వంటి సంస్థ AI అభివృద్ధి సాధనాలు మరియు అజూర్ AI సేవలను అందిస్తుందిఎయిడీ వర్క్‌ఫోర్స్ మైక్రోసాఫ్ట్ ఇంటెలిజెంట్ క్లౌడ్ హబ్‌ను ప్రకటించింది, ఇది తరువాతి తరం విద్యార్థులను ఎయిడీ స్కిల్స్ తో శక్తివంతం చేయడానికి ప్రారంభించబడింది\n",
      "త్రీఇయర్ సహకార కార్యక్రమంగా vision హించిన ఇంటెలిజెంట్ క్లౌడ్ హబ్ AI ఇన్ఫ్రాస్ట్రక్చర్ కోర్సు కంటెంట్ మరియు కరికులం డెవలపర్ డెవలప్‌మెంట్ డెవలప్‌మెంట్ సాధనాలతో సంస్థల చుట్టూ మద్దతు ఇస్తుంది మరియు విద్యార్థులకు క్లౌడ్ మరియు AI సేవలకు ప్రాప్తిని ఇస్తుంది\n",
      "ఈ కార్యక్రమంలో భాగంగా రెడ్‌మండ్ దిగ్గజం దాని పరిధిని విస్తరించాలని కోరుకుంటాడు మరియు ఈ కార్యక్రమంతో భారతదేశంలో బలమైన డెవలపర్ పర్యావరణ వ్యవస్థను నిర్మించాలని యోచిస్తోంది\n",
      "సంస్థ AI అభివృద్ధి సాధనాలు మరియు అజూర్ AI సేవలను మైక్రోసాఫ్ట్ కాగ్నిటివ్ సర్వీసెస్ బోట్ సర్వీసెస్ మరియు అజూర్ మెషిన్ లెర్నింగ్ఆర్కోర్డింగ్ టు మనీష్ ప్రకాష్ కంట్రీ జనరల్ మేనేజర్‌ప్స్ హెల్త్ అండ్ ఎడ్యుకేషన్ మైక్రోసాఫ్ట్ ఇండియా అన్నారు, AI మన సమయం యొక్క నిర్వచించే సాంకేతిక పరిజ్ఞానంరేపు ఉద్యోగాలకు వేరే నైపుణ్యం అవసరం\n",
      "దీనికి మరింత సహకారాలు మరియు శిక్షణ మరియు AI తో పనిచేయడం అవసరం\n",
      "కొత్త క్లౌడ్ మరియు AI టెక్నాలజీలను ఏకీకృతం చేయడానికి విద్యా సంస్థలకు ఇది గతంలో కంటే చాలా క్లిష్టంగా మారింది\n",
      "ఈ కార్యక్రమం సంస్థాగత సెటప్‌ను పెంచే ప్రయత్నం మరియు రేపు శ్రామిక శక్తిని విద్యావంతులను చేయడానికి విద్యావేత్తలలో సామర్థ్యాలను పెంపొందించే ప్రయత్నం, ఈ కార్యక్రమం పరిశ్రమ అంతటా అనువర్తనాల కోసం తెలివైన క్లౌడ్ కనెక్ట్ పరిష్కారాలను అభివృద్ధి చేయడంలో అభిజ్ఞా నైపుణ్యాలు మరియు అంతర్గత అవగాహనను పెంపొందించడం లక్ష్యంగా పెట్టుకుంది.\n",
      "ఈ సంవత్సరం ఏప్రిల్‌లో ఏప్రిల్‌లో కంపెనీ మైక్రోసాఫ్ట్ ప్రొఫెషనల్ ప్రోగ్రామ్‌ను AI లో ప్రకటించింది.\n",
      "AI మరియు డేటా సైన్స్ లలో వారి నైపుణ్యాలను మెరుగుపర్చాలని కోరుకునే ప్రోగ్రామర్‌లకు ఉద్యోగ రెడీ నైపుణ్యాలను అందించడానికి ఈ కార్యక్రమం అభివృద్ధి చేయబడింది, ఇందులో హ్యాండ్‌సన్ ల్యాబ్‌లు మరియు నిపుణుల బోధకులు కూడా ఉన్నారు\n",
      "ఈ కార్యక్రమంలో డెవలపర్‌ఫోకస్డ్ AI పాఠశాల కూడా ఉంది, ఇది AI నైపుణ్యాలను పెంపొందించడానికి కొంత ఆస్తులను అందించింది\n",
      "\n",
      "Hindi Translations:\n",
      "एक एयरएडी वर्कफोर्स बनाने के प्रयास में Microsoft ने इंटेलिजेंट क्लाउड हब की घोषणा की, जिसे अगली पीढ़ी के छात्रों की अगली पीढ़ी को सशक्त बनाने के लिए लॉन्च किया गया है\n",
      "एक तीन वर्षीय सहयोगी कार्यक्रम के रूप में कल्पना की गई इंटेलिजेंट क्लाउड हब एआई इन्फ्रास्ट्रक्चर कोर्स कंटेंट और पाठ्यक्रम डेवलपर के साथ संस्थानों के आसपास समर्थन करेगा और विकास उपकरणों का समर्थन करेगा और छात्रों को क्लाउड और एआई सेवाओं तक पहुंच प्रदान करेगा\n",
      "कार्यक्रम के हिस्से के रूप में रेडमंड दिग्गज जो अपनी पहुंच का विस्तार करना चाहता है और भारत में एक मजबूत डेवलपर पारिस्थितिकी तंत्र बनाने की योजना बना रहा है, कार्यक्रम के साथ चयनित परिसरों के लिए कोर एआई इन्फ्रास्ट्रक्चर और आईओटी हब स्थापित करेगा\n",
      "कंपनी Microsoft संज्ञानात्मक सेवा बॉट सर्विसेज और Azure मशीन लर्निंग के लिए AI डेवलपमेंट टूल्स और Azure AI सेवाएं प्रदान करेगी।एक एयरएडी वर्कफोर्स Microsoft ने इंटेलिजेंट क्लाउड हब की घोषणा की, जिसे अगली पीढ़ी के छात्रों की अगली पीढ़ी को सशक्त बनाने के लिए लॉन्च किया गया है\n",
      "एक तीन वर्षीय सहयोगी कार्यक्रम के रूप में कल्पना की गई इंटेलिजेंट क्लाउड हब एआई इन्फ्रास्ट्रक्चर कोर्स कंटेंट और पाठ्यक्रम डेवलपर के साथ संस्थानों के आसपास समर्थन करेगा और विकास उपकरणों का समर्थन करेगा और छात्रों को क्लाउड और एआई सेवाओं तक पहुंच प्रदान करेगा\n",
      "कार्यक्रम के हिस्से के रूप में रेडमंड दिग्गज जो अपनी पहुंच का विस्तार करना चाहता है और भारत में एक मजबूत डेवलपर पारिस्थितिकी तंत्र बनाने की योजना बना रहा है, कार्यक्रम के साथ चयनित परिसरों के लिए कोर एआई इन्फ्रास्ट्रक्चर और आईओटी हब स्थापित करेगा\n",
      "कंपनी Microsoft संज्ञानात्मक सेवा बॉट सर्विसेज और Azure मशीन लर्निंग के लिए AI डेवलपमेंट टूल्स और Azure AI सेवाएं प्रदान करेगी।कल की नौकरियों को एक अलग कौशल की आवश्यकता होगी\n",
      "इसके लिए अधिक सहयोग और प्रशिक्षण और एआई के साथ काम करने की आवश्यकता होगी\n",
      "यही कारण है कि यह शैक्षणिक संस्थानों के लिए नए क्लाउड और एआई प्रौद्योगिकियों को एकीकृत करने के लिए पहले से कहीं अधिक महत्वपूर्ण हो गया है\n",
      "कार्यक्रम संस्थागत सेटअप को रैंप करने और शिक्षकों के बीच क्षमताओं का निर्माण करने का एक प्रयास है, जो कल के कार्यबल को शिक्षित करने के लिए कार्यक्रम का उद्देश्य संज्ञानात्मक कौशल का निर्माण करना है और उद्योग भर में अनुप्रयोगों के लिए बुद्धिमान क्लाउड कनेक्टेड समाधानों को विकसित करने के लिए स्वतंत्र समझ का निर्माण करना है।\n",
      "इससे पहले इस साल अप्रैल में कंपनी ने AI में Microsoft पेशेवर कार्यक्रम की घोषणा की, जो जनता के लिए एक शिक्षण ट्रैक के रूप में खुला है\n",
      "कार्यक्रम को प्रोग्रामर को नौकरी के लिए तैयार कौशल प्रदान करने के लिए विकसित किया गया था, जो ऑनलाइन पाठ्यक्रमों की एक श्रृंखला के साथ एआई और डेटा विज्ञान में अपने कौशल को सुधारना चाहते थे, जिसमें हैंडसन लैब्स और विशेषज्ञ प्रशिक्षकों को भी शामिल किया गया था\n",
      "इस कार्यक्रम में डेवलपरफोकस्ड एआई स्कूल भी शामिल था जो एआई कौशल बनाने में मदद करने के लिए परिसंपत्तियों का एक समूह प्रदान करता था\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the file and read data\n",
    "with open(\"Text5.txt\", \"r\") as file:\n",
    "    filedata = file.readlines()\n",
    "\n",
    "# Assume the first paragraph is the first line\n",
    "article = filedata[0].split(\". \")  # Split sentences by \". \"\n",
    "sentences = [re.sub(\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in article]  # Clean and strip each sentence\n",
    "\n",
    "# Print original sentences and their translations\n",
    "print(\"Original Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Translate to Telugu\n",
    "telugu_sentences = translate_text(sentences, 'te')\n",
    "print(\"\\nTelugu Translations:\")\n",
    "for telugu_sentence in telugu_sentences:\n",
    "    print(telugu_sentence)\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_sentences = translate_text(sentences, 'hi')\n",
    "print(\"\\nHindi Translations:\")\n",
    "for hindi_sentence in hindi_sentences:\n",
    "    print(hindi_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0c60fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  ['In an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills', 'Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services', 'As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses', 'The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transIn an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills', 'Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services', 'As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses', 'The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transforming lives and industry and the jobs of tomorrow will require a different skillset', 'This will require more collaborations and training and working with AI', 'Thats why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies', 'The program is an attempt to ramp up the institutional setup and build capabilities among the educators to educate the workforce of tomorrow The program aims to build up the cognitive skills and indepth understanding of developing intelligent cloud connected solutions for applications across industry', 'Earlier in April this year the company announced Microsoft Professional Program In AI as a learning track open to the public', 'The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured handson labs and expert instructors as well', 'This program also included developerfocused AI school that provided a bunch of assets to help build AI skills']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6888209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e7f5d3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.96212683 0.97525199 0.98726313 0.96212683 0.97525199\n",
      "  0.97290342 0.91091855 0.98101646 0.97376451 0.94171354 0.9641545\n",
      "  0.9163155 ]\n",
      " [0.96212683 0.         0.95945941 0.96938409 1.         0.95945941\n",
      "  0.96796549 0.89183122 0.95684149 0.97903758 0.93809949 0.95866009\n",
      "  0.94036906]\n",
      " [0.97525199 0.95945941 0.         0.98064202 0.95945941 1.\n",
      "  0.9750388  0.9003052  0.96649964 0.97530959 0.95533876 0.98015981\n",
      "  0.93226067]\n",
      " [0.98726313 0.96938409 0.98064202 0.         0.96938409 0.98064202\n",
      "  0.99533422 0.9303422  0.98216058 0.97639005 0.96660264 0.97308821\n",
      "  0.93054036]\n",
      " [0.96212683 1.         0.95945941 0.96938409 0.         0.95945941\n",
      "  0.96796549 0.89183122 0.95684149 0.97903758 0.93809949 0.95866009\n",
      "  0.94036906]\n",
      " [0.97525199 0.95945941 1.         0.98064202 0.95945941 0.\n",
      "  0.9750388  0.9003052  0.96649964 0.97530959 0.95533876 0.98015981\n",
      "  0.93226067]\n",
      " [0.97290342 0.96796549 0.9750388  0.99533422 0.96796549 0.9750388\n",
      "  0.         0.93902882 0.97003276 0.97092223 0.96855485 0.97512306\n",
      "  0.9354457 ]\n",
      " [0.91091855 0.89183122 0.9003052  0.9303422  0.89183122 0.9003052\n",
      "  0.93902882 0.         0.91423732 0.90837398 0.93250237 0.90081092\n",
      "  0.84748025]\n",
      " [0.98101646 0.95684149 0.96649964 0.98216058 0.95684149 0.96649964\n",
      "  0.97003276 0.91423732 0.         0.97329636 0.93479555 0.94667835\n",
      "  0.90282645]\n",
      " [0.97376451 0.97903758 0.97530959 0.97639005 0.97903758 0.97530959\n",
      "  0.97092223 0.90837398 0.97329636 0.         0.94823265 0.95619257\n",
      "  0.93871916]\n",
      " [0.94171354 0.93809949 0.95533876 0.96660264 0.93809949 0.95533876\n",
      "  0.96855485 0.93250237 0.93479555 0.94823265 0.         0.94627942\n",
      "  0.89808157]\n",
      " [0.9641545  0.95866009 0.98015981 0.97308821 0.95866009 0.98015981\n",
      "  0.97512306 0.90081092 0.94667835 0.95619257 0.94627942 0.\n",
      "  0.95624531]\n",
      " [0.9163155  0.94036906 0.93226067 0.93054036 0.94036906 0.93226067\n",
      "  0.9354457  0.84748025 0.90282645 0.93871916 0.89808157 0.95624531\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e5ee3ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.07744208627243829, 1: 0.07721339783653264, 2: 0.07765558190618803, 3: 0.0781312671959768, 4: 0.07721339783653264, 5: 0.07765558190618803, 6: 0.07796752564238026, 7: 0.07365550702536447, 8: 0.07703085977400773, 9: 0.0776265373209134, 10: 0.07629144023602352, 11: 0.07728911463224962, 12: 0.07482770241520431}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3b2600a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.0781312671959768, 'The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transIn an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills'), (0.07796752564238026, 'The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services Bot Services and Azure Machine LearningAccording to Manish Prakash Country General ManagerPS Health and Education Microsoft India said With AI being the defining technology of our time it is transforming lives and industry and the jobs of tomorrow will require a different skillset'), (0.07765558190618803, 'As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses'), (0.07765558190618803, 'As part of the program the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses'), (0.0776265373209134, 'The program is an attempt to ramp up the institutional setup and build capabilities among the educators to educate the workforce of tomorrow The program aims to build up the cognitive skills and indepth understanding of developing intelligent cloud connected solutions for applications across industry'), (0.07744208627243829, 'In an attempt to build an AIready workforce Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AIready skills'), (0.07728911463224962, 'The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured handson labs and expert instructors as well'), (0.07721339783653264, 'Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services'), (0.07721339783653264, 'Envisioned as a threeyear collaborative program Intelligent Cloud Hub will support around  institutions with AI infrastructure course content and curriculum developer support development tools and give students access to cloud and AI services'), (0.07703085977400773, 'Thats why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies'), (0.07629144023602352, 'Earlier in April this year the company announced Microsoft Professional Program In AI as a learning track open to the public'), (0.07482770241520431, 'This program also included developerfocused AI school that provided a bunch of assets to help build AI skills'), (0.07365550702536447, 'This will require more collaborations and training and working with AI')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6938415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=2\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0a2a0938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " T h e   c o m p a n y   w i l l   p r o v i d e   A I   d e v e l o p m e n t   t o o l s   a n d   A z u r e   A I   s e r v i c e s   s u c h   a s   M i c r o s o f t   C o g n i t i v e   S e r v i c e s   B o t   S e r v i c e s   a n d   A z u r e   M a c h i n e   L e a r n i n g A c c o r d i n g   t o   M a n i s h   P r a k a s h   C o u n t r y   G e n e r a l   M a n a g e r P S   H e a l t h   a n d   E d u c a t i o n   M i c r o s o f t   I n d i a   s a i d   W i t h   A I   b e i n g   t h e   d e f i n i n g   t e c h n o l o g y   o f   o u r   t i m e   i t   i s   t r a n s I n   a n   a t t e m p t   t o   b u i l d   a n   A I r e a d y   w o r k f o r c e   M i c r o s o f t   a n n o u n c e d   I n t e l l i g e n t   C l o u d   H u b   w h i c h   h a s   b e e n   l a u n c h e d   t o   e m p o w e r   t h e   n e x t   g e n e r a t i o n   o f   s t u d e n t s   w i t h   A I r e a d y   s k i l l s. T h e   c o m p a n y   w i l l   p r o v i d e   A I   d e v e l o p m e n t   t o o l s   a n d   A z u r e   A I   s e r v i c e s   s u c h   a s   M i c r o s o f t   C o g n i t i v e   S e r v i c e s   B o t   S e r v i c e s   a n d   A z u r e   M a c h i n e   L e a r n i n g A c c o r d i n g   t o   M a n i s h   P r a k a s h   C o u n t r y   G e n e r a l   M a n a g e r P S   H e a l t h   a n d   E d u c a t i o n   M i c r o s o f t   I n d i a   s a i d   W i t h   A I   b e i n g   t h e   d e f i n i n g   t e c h n o l o g y   o f   o u r   t i m e   i t   i s   t r a n s f o r m i n g   l i v e s   a n d   i n d u s t r y   a n d   t h e   j o b s   o f   t o m o r r o w   w i l l   r e q u i r e   a   d i f f e r e n t   s k i l l s e t. A s   p a r t   o f   t h e   p r o g r a m   t h e   R e d m o n d   g i a n t   w h i c h   w a n t s   t o   e x p a n d   i t s   r e a c h   a n d   i s   p l a n n i n g   t o   b u i l d   a   s t r o n g   d e v e l o p e r   e c o s y s t e m   i n   I n d i a   w i t h   t h e   p r o g r a m   w i l l   s e t   u p   t h e   c o r e   A I   i n f r a s t r u c t u r e   a n d   I o T   H u b   f o r   t h e   s e l e c t e d   c a m p u s e s\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f9fc1d35-c06a-4460-a19b-ddfb21f3bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Summary:\n",
      "This is the first summarized sentence. This is the second summarized sentence.\n",
      "\n",
      "Telugu Summary:\n",
      "ఇది మొదటి సంగ్రహించిన వాక్యం.ఇది రెండవ సంగ్రహించిన వాక్యం.\n",
      "\n",
      "Hindi Summary:\n",
      "यह पहला सारांशित वाक्य है।यह दूसरा सारांशित वाक्य है।\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Assuming summarize_text is a list of strings (summarized sentences)\n",
    "summarize_text = [\"This is the first summarized sentence\", \"This is the second summarized sentence\"]\n",
    "\n",
    "# Join the list into a single string\n",
    "summarized_content = \". \".join(summarize_text) + \".\"\n",
    "\n",
    "def translate_summary(english_summary):\n",
    "    translator = Translator()\n",
    "    \n",
    "    # Translate the summary to Telugu\n",
    "    telugu_summary = translator.translate(english_summary, src='en', dest='te').text\n",
    "    print(\"\\nTelugu Summary:\")\n",
    "    print(telugu_summary)\n",
    "    \n",
    "    # Translate the summary to Hindi\n",
    "    hindi_summary = translator.translate(english_summary, src='en', dest='hi').text\n",
    "    print(\"\\nHindi Summary:\")\n",
    "    print(hindi_summary)\n",
    "\n",
    "# Print the English summary\n",
    "print(\"English Summary:\")\n",
    "print(summarized_content)\n",
    "\n",
    "# Translate the summary\n",
    "translate_summary(summarized_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c549e-54e7-47b9-a8b9-882ebdc2bd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
